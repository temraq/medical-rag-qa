from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, BitsAndBytesConfig
from peft import PeftModel
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from datetime import datetime
import logging
import os
import time
from google.cloud import storage
from google.auth.exceptions import DefaultCredentialsError

# üéØ –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.FileHandler("/tmp/api.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("RAG-API")
app = FastAPI()

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Cloud Run
IS_CLOUD_RUN = os.environ.get("K_SERVICE") is not None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (–ø—É—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ GCS)
GCS_BUCKET_NAME = os.environ.get("GCS_BUCKET_NAME", "")
MODEL_GCS_PATH = os.environ.get("MODEL_GCS_PATH", "zephyr-7b-base")
ADAPTERS_GCS_PATH = os.environ.get("ADAPTERS_GCS_PATH", "zephyr-medical-adapter")
INDEX_GCS_PATH = os.environ.get("INDEX_GCS_PATH", "pubmed-rag-index")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
REQUIRED_ENV_VARS = ["GCS_BUCKET_NAME"]
missing_vars = [var for var in REQUIRED_ENV_VARS if not os.environ.get(var)]
if missing_vars and IS_CLOUD_RUN:
    error_msg = f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing_vars)}"
    logger.error(error_msg)
    raise EnvironmentError(error_msg)

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
BASE_DIR = "/app"
MODEL_LOCAL_PATH = f"{BASE_DIR}/models/zephyr_base_model"
ADAPTERS_LOCAL_PATH = f"{BASE_DIR}/models/zephyr_medical_rag_adapter"
INDEX_LOCAL_PATH = f"{BASE_DIR}/index/pubmed_rag_index"

EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
MIN_RELEVANCE_THRESHOLD = 0.55
MAX_CONTEXT_LENGTH = 512 if not IS_CLOUD_RUN else 384  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è Cloud Run

logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ RAG API —Å–µ—Ä–≤–µ—Ä–∞ –≤ —Ä–µ–∂–∏–º–µ: {'Cloud Run' if IS_CLOUD_RUN else 'Local'}")
logger.info(f"üì¶ Bucket: {GCS_BUCKET_NAME}, Model path: {MODEL_GCS_PATH}")

def download_from_gcs(bucket_name, gcs_path, local_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ Google Cloud Storage —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Python SDK"""
    if not bucket_name:
        logger.warning("GCS_BUCKET_NAME –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É")
        return False
    
    try:
        os.makedirs(local_path, exist_ok=True)
        logger.info(f"‚¨áÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ gs://{bucket_name}/{gcs_path} –≤ {local_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ GCS
        try:
            storage_client = storage.Client()
            logger.info("‚úÖ GCS –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except DefaultCredentialsError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ GCS: {e}")
            logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–æ–Ω–∏–º–Ω—ã–π –¥–æ—Å—Ç—É–ø...")
            storage_client = storage.Client.create_anonymous_client()
        
        bucket = storage_client.bucket(bucket_name)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        blobs = bucket.list_blobs(prefix=f"{gcs_path}/")
        total_files = 0
        downloaded_files = 0
        
        start_time = time.time()
        
        for blob in blobs:
            total_files += 1
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if blob.name.endswith('/'):
                continue
                
            # –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å
            relative_path = blob.name[len(gcs_path):].lstrip('/')
            local_file_path = os.path.join(local_path, relative_path)
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {blob.name} -> {local_file_path}")
            blob.download_to_filename(local_file_path)
            downloaded_files += 1
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {local_file_path}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {downloaded_files}/{total_files} —Ñ–∞–π–ª–æ–≤ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        return downloaded_files > 0
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ GCS: {e}", exc_info=True)
        return False

def ensure_models_available():
    """–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –∏–Ω–¥–µ–∫—Å–∞"""
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π –∏ –∏–Ω–¥–µ–∫—Å–∞...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
    if not os.path.exists(MODEL_LOCAL_PATH) or not os.listdir(MODEL_LOCAL_PATH):
        logger.info("üîÑ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ GCS")
        success = download_from_gcs(GCS_BUCKET_NAME, MODEL_GCS_PATH, MODEL_LOCAL_PATH)
        if not success:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã
    if not os.path.exists(ADAPTERS_LOCAL_PATH) or not os.listdir(ADAPTERS_LOCAL_PATH):
        logger.info("üîÑ –ê–¥–∞–ø—Ç–µ—Ä—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ GCS")
        success = download_from_gcs(GCS_BUCKET_NAME, ADAPTERS_GCS_PATH, ADAPTERS_LOCAL_PATH)
        if not success:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–¥–∞–ø—Ç–µ—Ä—ã")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
    if not os.path.exists(INDEX_LOCAL_PATH) or not os.listdir(INDEX_LOCAL_PATH):
        logger.info("üîÑ FAISS –∏–Ω–¥–µ–∫—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ GCS")
        success = download_from_gcs(GCS_BUCKET_NAME, INDEX_GCS_PATH, INDEX_LOCAL_PATH)
        if not success:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    paths = {
        "–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å": MODEL_LOCAL_PATH,
        "–∞–¥–∞–ø—Ç–µ—Ä—ã": ADAPTERS_LOCAL_PATH,
        "–∏–Ω–¥–µ–∫—Å": INDEX_LOCAL_PATH
    }
    
    missing = [name for name, path in paths.items() if not os.path.exists(path) or not os.listdir(path)]
    
    if missing:
        error_msg = f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {', '.join(missing)}"
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º API
logger.info("‚è≥ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π...")
start_time = time.time()
ensure_models_available()
logger.info(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫—É–Ω–¥")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
tokenizer = None
model = None
vector_db = None

logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_PATH)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω.")

logger.info("\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ 4-bit –¥–ª—è Cloud Run...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Cloud Run - –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU offload –∏ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
model = AutoModelForCausalLM.from_pretrained(
    MODEL_LOCAL_PATH,
    quantization_config=bnb_config,
    device_map="auto",
    offload_folder="/tmp/offload",
    trust_remote_code=False,
    use_cache=True,
    low_cpu_mem_usage=True
)

logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
model = PeftModel.from_pretrained(model, ADAPTERS_LOCAL_PATH)
model.eval()
logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∏ –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

logger.info("üåê –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

logger.info(f"Loading FAISS index from {INDEX_LOCAL_PATH}...")
vector_db = FAISS.load_local(
    INDEX_LOCAL_PATH,
    embedding_model,
    allow_dangerous_deserialization=True
)
logger.info("‚úÖ FAISS index loaded successfully.")

# –§—É–Ω–∫—Ü–∏—è smart_retrieve (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def smart_retrieve(query, k=3):
    base_results = vector_db.similarity_search_with_relevance_scores(query, k=20)
    doc_to_score = {doc.metadata["id"]: score for doc, score in base_results}

    query_lower = query.lower()
    medical_guideline_keywords = ["guideline", "standard", "recommendation", "protocol", "consensus", "algorithm"]
    diagnostic_keywords = ["diagnos", "criteria", "symptom", "sign", "test", "screening"]
    treatment_keywords = ["treat", "therapy", "management", "intervention", "medication", "drug"]

    is_guideline = any(kw in query_lower for kw in medical_guideline_keywords)
    is_diagnostic = any(kw in query_lower for kw in diagnostic_keywords)
    is_treatment = any(kw in query_lower for kw in treatment_keywords)
    token_count = len(tokenizer.encode(query))

    if is_guideline:
        lam = 0.98
        reason = "–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"
    elif is_diagnostic or is_treatment:
        lam = 0.88
        reason = "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞/–ª–µ—á–µ–Ω–∏–µ"
    elif token_count > 40:
        lam = 0.75
        reason = "–î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (>40 —Ç–æ–∫–µ–Ω–æ–≤)"
    else:
        lam = 0.82
        reason = "–û–±—â–∏–π –∑–∞–ø—Ä–æ—Å"

    logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {reason}, lambda = {lam:.2f}")

    mmr_results = vector_db.max_marginal_relevance_search(
        query,
        k=k,
        lambda_mult=lam,
        fetch_k=20
    )

    unique_results = {}
    for doc in mmr_results:
        pub_id = doc.metadata.get("pubmed_id", doc.metadata.get("id", "unknown"))
        if pub_id in unique_results:
            continue
        score = doc_to_score.get(doc.metadata["id"], 0.0)
        unique_results[pub_id] = (doc, score)
        if len(unique_results) >= k:
            break

    sorted_results = sorted(unique_results.values(), key=lambda x: x[1], reverse=True)
    docs = [item[0] for item in sorted_results]
    scores = [item[1] for item in sorted_results]

    return docs, scores

# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def format_zephyr_rag_messages(query, retrieved_docs):
    context_parts = []
    for i, doc in enumerate(retrieved_docs):
        metadata = doc.metadata or {}
        source_info = f"[Source {i+1}]"
        if "title" in metadata:
            source_info += f" '{metadata['title']}'"
        if "pubmed_id" in metadata:
            source_info += f" (PMID: {metadata['pubmed_id']})"
        content = doc.page_content if hasattr(doc, "page_content") else getattr(doc, "content", str(doc))
        context_parts.append({"source": source_info, "content": content, "metadata": metadata})

    assembled_context = "\n\n".join(f"{p['source']}\n{p['content']}" for p in context_parts)

    messages = [
        {
            "role": "system",
            "content": (
                "You are a medical expert analyzing evidence. Your task is to:"
                " 1. Identify specific findings in the context relevant to the question"
                " 2. Note statistical significance (p-values, confidence intervals) when present"
                " 3. Reference source numbers when making claims"
                " 4. If answer isn't explicit, state 'I cannot provide a definitive answer'"
                " Do NOT use prior knowledge - base your answer ONLY on the provided context."
                f" Current date: {datetime.now().strftime('%Y-%m-%d')}"
            )
        },
        {
            "role": "user",
            "content": f"Medical Context:\n{assembled_context}\n\nQuestion: {query}"
        }
    ]
    return messages, assembled_context, context_parts

# –ö–ª–∞—Å—Å –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
class StopOnSubsequences(StoppingCriteria):
    def __init__(self, stop_sequences_ids):
        self.stop_sequences_ids = [seq for seq in stop_sequences_ids if seq]

    def _ends_with(self, haystack, needle):
        n = len(needle)
        if n == 0 or len(haystack) < n:
            return False
        return haystack[-n:] == needle

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        last = input_ids[0].tolist()
        for seq in self.stop_sequences_ids:
            if self._ends_with(last, seq):
                return True
        return False

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è Cloud Run)
def zephyr_rag_pipeline(query: str, k: int = 3, max_new_tokens: int = 350, min_relevance: float = MIN_RELEVANCE_THRESHOLD):
    if IS_CLOUD_RUN:
        k = min(k, 2)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Cloud Run
        max_new_tokens = min(max_new_tokens, 150)  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

    retrieved_docs, scores = smart_retrieve(query, k=k)
    if not scores or max(scores) < min_relevance:
        return {
            "answer": "Based on the available medical sources, I cannot give a definitive answer to this question.",
            "confidence": "low",
            "retrieved_docs": [],
            "scores": scores,
            "assembled_context": "",
            "context_parts": []
        }

    messages, assembled_context, context_parts = format_zephyr_rag_messages(query, retrieved_docs)
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_CONTEXT_LENGTH,
        padding=True
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    if tokenizer.pad_token_id is None:
        tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})

    eos_token_id = tokenizer.eos_token_id

    stop_markers = [
        "<|end|>", "<|user|>", "<|assistant|>", "</s>",
        "<| user|>", "<|Assistant|>", "<||user|>", "<|User|>"
    ]
    stop_ids_list = []
    for sm in stop_markers:
        try:
            ids = tokenizer.encode(sm, add_special_tokens=False)
            if ids:
                stop_ids_list.append(ids)
        except Exception:
            continue

    stopping_criteria = StoppingCriteriaList([StopOnSubsequences(stop_ids_list)])
    max_new_tokens = min(max_new_tokens, 256)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.2,
            top_p=0.8,
            do_sample=False,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=eos_token_id,
            stopping_criteria=stopping_criteria,
        )

    full_ids = outputs[0].tolist()
    prompt_len = inputs["input_ids"].shape[1]
    gen_ids = full_ids[prompt_len:]

    assistant_ids = tokenizer.encode("<|assistant|>", add_special_tokens=False)
    def find_subsequence_last(haystack, needle):
        if not needle:
            return -1
        n = len(needle)
        for i in range(len(haystack) - n, -1, -1):
            if haystack[i:i + n] == needle:
                return i
        return -1

    start_idx = 0
    if assistant_ids:
        pos = find_subsequence_last(gen_ids, assistant_ids)
        if pos != -1:
            start_idx = pos + len(assistant_ids)

    end_idx = None
    for stop_ids in stop_ids_list:
        n = len(stop_ids)
        for i in range(start_idx, len(gen_ids) - n + 1):
            if gen_ids[i:i + n] == stop_ids:
                if end_idx is None or i < end_idx:
                    end_idx = i
                break

    if end_idx is None:
        answer_ids = gen_ids[start_idx:]
    else:
        answer_ids = gen_ids[start_idx:end_idx]

    answer = tokenizer.decode(answer_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()

    end_marker_phrase = "This conclusion is based strictly on the specific medical evidence provided in the context."
    low_ans = answer.lower()
    if end_marker_phrase.lower() in low_ans:
        idx = low_ans.index(end_marker_phrase.lower()) + len(end_marker_phrase)
        answer = answer[:idx].strip()

    first_pos = None
    for sm in stop_markers:
        pos = answer.find(sm)
        if pos != -1:
            if first_pos is None or pos < first_pos:
                first_pos = pos
    if first_pos is not None:
        answer = answer[:first_pos].strip()

    if not answer:
        answer = "Based on the available medical sources, I cannot give a definitive answer to this question."

    context_used = False
    try:
        for i in range(1, k + 1):
            if f"source {i}" in answer.lower() or f"[source {i}]" in answer.lower():
                context_used = True
                break
    except Exception:
        context_used = False

    confidence = "high" if (max(scores) > 0.75 and context_used) else "medium"
    full_generated_text = tokenizer.decode(gen_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)

    return {
        "answer": answer,
        "confidence": confidence,
        "retrieved_docs": retrieved_docs,
        "scores": scores,
        "full_generated_text": full_generated_text,
        "context_used": context_used,
        "assembled_context": assembled_context,
        "context_parts": context_parts
    }

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è API
class QueryRequest(BaseModel):
    query: str
    k: int = 3
    min_relevance: float = MIN_RELEVANCE_THRESHOLD

@app.post("/query")
async def query_rag(request: QueryRequest):
    try:
        logger.info(f"üîç –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {request.query}")
        start_time = time.time()
        
        result = zephyr_rag_pipeline(
            query=request.query,
            k=request.k,
            min_relevance=request.min_relevance
        )
        
        processing_time = time.time() - start_time
        logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥, confidence: {result['confidence']}")
        
        return result
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint for Cloud Run"""
    return {
        "status": "healthy",
        "models_loaded": all([
            tokenizer is not None,
            model is not None,
            vector_db is not None
        ]),
        "environment": "Cloud Run" if IS_CLOUD_RUN else "Local",
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8081))
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º startup event –¥–ª—è –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    @app.on_event("startup")
    async def startup_event():
        logger.info("‚úÖ API —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=port,
        log_level="info",
        timeout_keep_alive=300  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è Cloud Run
    )