from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, BitsAndBytesConfig
from peft import PeftModel
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from datetime import datetime
import logging
import time
from tqdm import tqdm
from accelerate import infer_auto_device_map, init_empty_weights
from transformers import AutoConfig


# üéØ –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.FileHandler("/app/logs/api.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("RAG-API")
app = FastAPI()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
MODEL_NAME = "models/zephyr_base_model"
ADAPTERS_PATH = "models/zephyr_medical_rag_adapter"
INDEX_PATH = "index/pubmed_rag_index"
EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
MIN_RELEVANCE_THRESHOLD = 0.55
MAX_CONTEXT_LENGTH = 512

logger.info("Starting RAG API server...")

def create_progress_bar(total_steps, desc="Loading"):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏"""
    return tqdm(total=total_steps, desc=desc, unit="step", colour='green')

try:
    logger.info("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    pbar_tokenizer = create_progress_bar(2, desc="Loading tokenizer")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    pbar_tokenizer.update(1)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    pbar_tokenizer.update(1)
    pbar_tokenizer.close()
    
    logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω.")
    logger.info("\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ CPU —Å offloading –Ω–∞ –¥–∏—Å–∫...")
    pbar_model = create_progress_bar(3, desc="Loading model")

    config = AutoConfig.from_pretrained(MODEL_NAME)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ RAM –¥–æ—Å—Ç—É–ø–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 6 –ì–ë –¥–ª—è –º–æ–¥–µ–ª–∏, –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî —Å–∏—Å—Ç–µ–º–µ)
    # –ï—Å–ª–∏ —É –≤–∞—Å –º–∞–ª–æ RAM (<16 –ì–ë), —É–º–µ–Ω—å—à–∏—Ç–µ "cpu" –¥–æ "4GiB"
    max_memory = {
        "cpu": "4GiB",      # ‚Üê –ø–æ–¥—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–¥ –≤–∞—à—É —Å–∏—Å—Ç–µ–º—É
        "disk": "20GiB"     # –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è "–ø–∞–º—è—Ç—å" –Ω–∞ –¥–∏—Å–∫–µ
    }

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ—ë–≤ –º–µ–∂–¥—É CPU –∏ –¥–∏—Å–∫–æ–º
    device_map = infer_auto_device_map(
        AutoModelForCausalLM.from_config(config),
        max_memory=max_memory,
        no_split_module_classes=["LlamaDecoderLayer"],
        dtype=torch.float16  # –∏–ª–∏ torch.bfloat16, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    )

    logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω device_map: {device_map}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ë–ï–ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map=device_map,
        offload_folder="offload",
        offload_state_dict=True,
        torch_dtype=torch.float16,  # —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ vs float32
        low_cpu_mem_usage=True,
        trust_remote_code=False
    )
    pbar_model.update(2)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã (LoRA) ‚Äî –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ CPU
    logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    model = PeftModel.from_pretrained(model, ADAPTERS_PATH)
    model.eval()
    pbar_model.update(1)
    pbar_model.close()
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∏ –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ CPU —Å offloading.")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    logger.info("üåê –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    pbar_embeddings = create_progress_bar(2, desc="Loading embeddings")
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    pbar_embeddings.update(1)
    logger.info("Loading FAISS index from %s...", INDEX_PATH)
    pbar_faiss = create_progress_bar(1, desc="Loading FAISS index")
    vector_db = FAISS.load_local(
        INDEX_PATH,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    pbar_faiss.update(1)
    pbar_faiss.close()
    pbar_embeddings.close()
    
except Exception as e:
    logger.exception(f"Initialization failed: {e}")  # ‚Üê –ª—É—á—à–µ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å traceback

# –§—É–Ω–∫—Ü–∏—è smart_retrieve (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è CPU)
def smart_retrieve(query, k=3):
    """
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - query: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    - k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –°–ø–∏—Å–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å–∫–æ—Ä–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    """
    # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏

    base_results = vector_db.similarity_search_with_relevance_scores(query, k=20)
    doc_to_score = {doc.metadata["id"]: score for doc, score in base_results}

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä lambda
    query_lower = query.lower()

  # 1. –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
    medical_guideline_keywords = ["guideline", "standard", "recommendation", "protocol", "consensus", "algorithm"]
    diagnostic_keywords = ["diagnos", "criteria", "symptom", "sign", "test", "screening"]
    treatment_keywords = ["treat", "therapy", "management", "intervention", "medication", "drug"]

    is_guideline = any(kw in query_lower for kw in medical_guideline_keywords)
    is_diagnostic = any(kw in query_lower for kw in diagnostic_keywords)
    is_treatment = any(kw in query_lower for kw in treatment_keywords)

  # 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤–º–µ—Å—Ç–æ —Å–ª–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–ª–∏–Ω—ã
    token_count = len(tokenizer.encode(query))

    # 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è lambda –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
    if is_guideline:
        # –î–ª—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω—É–∂–Ω–∞ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø —Ç–æ—á–Ω–æ—Å—Ç—å
        # 0.98 –æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ–≥–æ 2% –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (–º–∏–Ω–∏–º—É–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ)
        lam = 0.98
        reason = "–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"

    elif is_diagnostic or is_treatment:
        # –î–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–µ—á–µ–±–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–∞–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ –Ω—É–∂–µ–Ω –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        lam = 0.88
        reason = "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞/–ª–µ—á–µ–Ω–∏–µ"

    elif token_count > 40:  # –ë–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–π –ø–æ—Ä–æ–≥ (40 —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ 8 —Å–ª–æ–≤)
        # –î–ª—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        lam = 0.75
        reason = "–î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (>40 —Ç–æ–∫–µ–Ω–æ–≤)"

    else:
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –æ–±—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        lam = 0.82
        reason = "–û–±—â–∏–π –∑–∞–ø—Ä–æ—Å"

    print(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {reason}, lambda = {lam:.2f}")

    # –£–ª—É—á—à–µ–Ω–Ω—ã–π MMR —Å –±–æ–ª—å—à–∏–º fetch_k –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    mmr_results = vector_db.max_marginal_relevance_search(
        query,
        k=k,
        lambda_mult=lam,
        fetch_k=20  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã–±–æ—Ä–∞
    )

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏–∑ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ (–º–∞–∫—Å 1 —á–∞–Ω–∫ –Ω–∞ —Å—Ç–∞—Ç—å—é)
    unique_results = {}
    for doc in mmr_results:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º pubmed_id –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏
        pub_id = doc.metadata.get("pubmed_id", doc.metadata.get("id", "unknown"))

        # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if pub_id in unique_results:
            continue

        # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        score = doc_to_score.get(doc.metadata["id"], 0.0)
        unique_results[pub_id] = (doc, score)

        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º, –∫–æ–≥–¥–∞ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        if len(unique_results) >= k:
            break

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–∫–æ—Ä—É –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
    sorted_results = sorted(unique_results.values(), key=lambda x: x[1], reverse=True)

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–∫–æ—Ä—ã
    docs = [item[0] for item in sorted_results]
    scores = [item[1] for item in sorted_results]

    return docs, scores

# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
def format_zephyr_rag_messages(query, retrieved_docs):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (messages, assembled_context_str, context_parts_list)"""
    context_parts = []
    for i, doc in enumerate(retrieved_docs):
        metadata = doc.metadata or {}
        source_info = f"[Source {i+1}]"
        if "title" in metadata:
            source_info += f" '{metadata['title']}'"
        if "pubmed_id" in metadata:
            source_info += f" (PMID: {metadata['pubmed_id']})"

        content = doc.page_content if hasattr(doc, "page_content") else getattr(doc, "content", str(doc))
        # –Ω–µ —É—Å–µ–∫–∞—Ç—å –∑–¥–µ—Å—å ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –∫—É—Å–æ–∫; —Ç—Ä–∏–º–∏—Ç—å –º–æ–∂–Ω–æ –ø—Ä–∏ –ø–µ—á–∞—Ç–∏
        context_parts.append({"source": source_info, "content": content, "metadata": metadata})

    # —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è / —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)
    assembled_context = "\n\n".join(f"{p['source']}\n{p['content']}" for p in context_parts)

    messages = [
        {
            "role": "system",
            "content": (
                "You are a medical expert analyzing evidence. Your task is to:"
                " 1. Identify specific findings in the context relevant to the question"
                " 2. Note statistical significance (p-values, confidence intervals) when present"
                " 3. Reference source numbers when making claims"
                " 4. If answer isn't explicit, state 'I cannot provide a definitive answer'"
                " Do NOT use prior knowledge - base your answer ONLY on the provided context."
                f" Current date: {datetime.now().strftime('%Y-%m-%d')}"
            )
        },
        {
            "role": "user",
            "content": f"Medical Context:\n{assembled_context}\n\nQuestion: {query}"
        }
    ]
    return messages, assembled_context, context_parts

# –ö–ª–∞—Å—Å –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
class StopOnSubsequences(StoppingCriteria):
    """
    –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
    –æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ª—é–±—É—é –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤.
    """
    def __init__(self, stop_sequences_ids):
        # stop_sequences_ids: list of list of ints
        self.stop_sequences_ids = [seq for seq in stop_sequences_ids if seq]  # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ

    def _ends_with(self, haystack, needle):
        n = len(needle)
        if n == 0 or len(haystack) < n:
            return False
        return haystack[-n:] == needle

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        # –†–∞–±–æ—Ç–∞–µ—Ç —Å –ø–µ—Ä–≤—ã–º (–∏ –æ–±—ã—á–Ω–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º) —ç–ª–µ–º–µ–Ω—Ç–æ–º –±–∞—Ç—á–∞
        last = input_ids[0].tolist()
        for seq in self.stop_sequences_ids:
            if self._ends_with(last, seq):
                return True
        return False

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞
def zephyr_rag_pipeline(query: str, k: int = 3, max_new_tokens: int = 350, min_relevance: float = MIN_RELEVANCE_THRESHOLD):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: answer, confidence, retrieved_docs, scores, full_generated_text, context_used,
                assembled_context (—Å—Ç—Ä–æ–∫–∞), context_parts (list of dicts with source/content/metadata)
    """
    retrieved_docs, scores = smart_retrieve(query, k=k)
    if not scores or max(scores) < min_relevance:
        return {
            "answer": "Based on the available medical sources, I cannot give a definitive answer to this question.",
            "confidence": "low",
            "retrieved_docs": [],
            "scores": scores,
            "assembled_context": "",
            "context_parts": []
        }

    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–ø–µ—Ä—å —Ç—Ä–æ–π–∫—É: messages, assembled_context –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏
    messages, assembled_context, context_parts = format_zephyr_rag_messages(query, retrieved_docs)
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º pad token
    if tokenizer.pad_token_id is None:
        tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})

    eos_token_id = tokenizer.eos_token_id

    # —Å–±–æ—Ä —Å—Ç–æ–ø-–º–∞—Ä–∫–µ—Ä–æ–≤ –∫–∞–∫ —É –≤–∞—Å –±—ã–ª
    stop_markers = [
        "<|end|>", "<|user|>", "<|assistant|>", "</s>",
        "<| user|>", "<|Assistant|>", "<||user|>", "<|User|>"
    ]
    stop_ids_list = []
    for sm in stop_markers:
        try:
            ids = tokenizer.encode(sm, add_special_tokens=False)
            if ids:
                stop_ids_list.append(ids)
        except Exception:
            continue

    stopping_criteria = StoppingCriteriaList([StopOnSubsequences(stop_ids_list)])
    max_new_tokens = min(max_new_tokens, 256)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.2,
            top_p=0.8,
            do_sample=False,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=eos_token_id,
            stopping_criteria=stopping_criteria,
        )

    full_ids = outputs[0].tolist()
    prompt_len = inputs["input_ids"].shape[1]
    gen_ids = full_ids[prompt_len:]

    assistant_ids = tokenizer.encode("<|assistant|>", add_special_tokens=False)
    def find_subsequence_last(haystack, needle):
        if not needle:
            return -1
        n = len(needle)
        for i in range(len(haystack) - n, -1, -1):
            if haystack[i:i + n] == needle:
                return i
        return -1

    start_idx = 0
    if assistant_ids:
        pos = find_subsequence_last(gen_ids, assistant_ids)
        if pos != -1:
            start_idx = pos + len(assistant_ids)

    # find nearest stop
    end_idx = None
    for stop_ids in stop_ids_list:
        n = len(stop_ids)
        for i in range(start_idx, len(gen_ids) - n + 1):
            if gen_ids[i:i + n] == stop_ids:
                if end_idx is None or i < end_idx:
                    end_idx = i
                break

    if end_idx is None:
        answer_ids = gen_ids[start_idx:]
    else:
        answer_ids = gen_ids[start_idx:end_idx]

    answer = tokenizer.decode(answer_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()

    # –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ —É –≤–∞—Å –±—ã–ª–∞
    end_marker_phrase = "This conclusion is based strictly on the specific medical evidence provided in the context."
    low_ans = answer.lower()
    if end_marker_phrase.lower() in low_ans:
        idx = low_ans.index(end_marker_phrase.lower()) + len(end_marker_phrase)
        answer = answer[:idx].strip()

    first_pos = None
    for sm in stop_markers:
        pos = answer.find(sm)
        if pos != -1:
            if first_pos is None or pos < first_pos:
                first_pos = pos
    if first_pos is not None:
        answer = answer[:first_pos].strip()

    if not answer:
        answer = "Based on the available medical sources, I cannot give a definitive answer to this question."

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    context_used = False
    try:
        for i in range(1, k + 1):
            if f"source {i}" in answer.lower() or f"[source {i}]" in answer.lower():
                context_used = True
                break
    except Exception:
        context_used = False

    confidence = "high" if (max(scores) > 0.75 and context_used) else "medium"
    full_generated_text = tokenizer.decode(gen_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)

    return {
        "answer": answer,
        "confidence": confidence,
        "retrieved_docs": retrieved_docs,
        "scores": scores,
        "full_generated_text": full_generated_text,
        "context_used": context_used,
        "assembled_context": assembled_context,
        "context_parts": context_parts
    }

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è API
class QueryRequest(BaseModel):
    query: str
    k: int = 3
    min_relevance: float = MIN_RELEVANCE_THRESHOLD

@app.post("/query")
async def query_rag(request: QueryRequest):
    try:
        result = zephyr_rag_pipeline(
            query=request.query,
            k=request.k,
            min_relevance=request.min_relevance
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)